"""
–°—Ç–µ–∫–∏–Ω–≥ –Ω–∞ –æ—Å–Ω–æ–≤–µ Best —Ñ–∞–π–ª–æ–≤
"""

import numpy as np
import pandas as pd
import lightgbm as lgb
import xgboost as xgb
from catboost import CatBoostClassifier
from sklearn.preprocessing import RobustScaler, StandardScaler
from sklearn.model_selection import StratifiedKFold
from sklearn.metrics import roc_auc_score
from sklearn.linear_model import LogisticRegression
from sklearn.pipeline import Pipeline
from sklearn.base import BaseEstimator, TransformerMixin
import warnings
warnings.filterwarnings('ignore')


# ============================================================================
# DATA COLLECTION & SPLIT
# ============================================================================

def get_input():
    """–ó–∞–≥—Ä—É–∑–∫–∞ –∏ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö"""
    import os
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø—É—Ç—å –∫ –¥–∞–Ω–Ω—ã–º –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ —Ç–µ–∫—É—â–µ–≥–æ —Ñ–∞–π–ª–∞
    base_dir = os.path.dirname(os.path.abspath(__file__))
    data_dir = os.path.join(os.path.dirname(base_dir), 'playground-series-s3e24')
    
    train = pd.read_csv(os.path.join(data_dir, 'train.csv'))
    test = pd.read_csv(os.path.join(data_dir, 'test.csv'))

    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ id –¥–ª—è submission
    train_ids = train['id'].copy()
    test_ids = test['id'].copy()

    # –í—ã–¥–µ–ª–µ–Ω–∏–µ —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π
    y_train = train['smoking'].copy()

    # –£–¥–∞–ª–µ–Ω–∏–µ id –∏ —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π –∏–∑ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    X_train = train.drop(['id', 'smoking'], axis=1)
    X_test = test.drop(['id'], axis=1)

    return X_train, y_train, X_test, test_ids


# ============================================================================
# FEATURE ENGINEERING (—Ç–∞ –∂–µ –ª–æ–≥–∏–∫–∞, —á—Ç–æ –≤ Best —Ñ–∞–π–ª–∞—Ö)
# ============================================================================

def create_extra_features(df):
    """–°–æ–∑–¥–∞–Ω–∏–µ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤"""
    # order the ears
    best = np.where(df['hearing(left)'] < df['hearing(right)'],
                    df['hearing(left)'],  df['hearing(right)'])
    worst = np.where(df['hearing(left)'] < df['hearing(right)'],
                     df['hearing(right)'],  df['hearing(left)'])
    df['hearing(left)'] = best - 1
    df['hearing(right)'] = worst - 1

    # order the eyes - eyesight is worst to best, and 9+ should be worst!
    df['eyesight(left)'] = np.where(df['eyesight(left)'] > 9, 0, df['eyesight(left)'])
    df['eyesight(right)'] = np.where(df['eyesight(right)'] > 9, 0, df['eyesight(right)'])
    best = np.where(df['eyesight(left)'] < df['eyesight(right)'],
                    df['eyesight(left)'],  df['eyesight(right)'])
    worst = np.where(df['eyesight(left)'] < df['eyesight(right)'],
                     df['eyesight(right)'],  df['eyesight(left)'])
    df['eyesight(left)'] = best
    df['eyesight(right)'] = worst
    ##
    df['Gtp'] = np.clip(df['Gtp'], 0, 300)
    df['HDL'] = np.clip(df['HDL'], 0, 110)
    df['LDL'] = np.clip(df['LDL'], 0, 200)
    df['ALT'] = np.clip(df['ALT'], 0, 150)
    df['AST'] = np.clip(df['AST'], 0, 100)
    df['serum creatinine'] = np.clip(df['serum creatinine'], 0, 3)


def category_encoding(train_category, test_category):
    """One hot encoding –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤"""
    train_encode = pd.get_dummies(train_category, columns=['hearing(left)', 'hearing(right)', 'Urine protein', 'dental caries'])
    test_encode = pd.get_dummies(test_category, columns=['hearing(left)', 'hearing(right)', 'Urine protein', 'dental caries'])
    return train_encode, test_encode


class DataPreprocessor(BaseEstimator, TransformerMixin):
    """–¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä –¥–ª—è –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–∞–Ω–Ω—ã—Ö (–¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤ Pipeline)"""
    
    def __init__(self, scaler=None, category_cols=None):
        self.scaler = scaler
        self.category_cols = category_cols
        self.fitted_scaler_ = None
        
    def fit(self, X, y=None):
        if self.scaler is None:
            self.fitted_scaler_ = RobustScaler()
        else:
            self.fitted_scaler_ = self.scaler
        
        # –ü—Ä–∏–º–µ–Ω—è–µ–º create_extra_features
        X_copy = X.copy()
        create_extra_features(X_copy)
        
        # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ –∏ —á–∏—Å–ª–æ–≤—ã–µ
        train_to_scale = X_copy.drop(self.category_cols, axis=1)
        self.fitted_scaler_.fit(train_to_scale)
        
        return self
    
    def transform(self, X):
        X_copy = X.copy()
        create_extra_features(X_copy)
        
        # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ –∏ —á–∏—Å–ª–æ–≤—ã–µ
        X_to_scale = X_copy.drop(self.category_cols, axis=1)
        X_category = X_copy[self.category_cols]
        
        # –°—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∞—Ü–∏—è —á–∏—Å–ª–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        scaled_X = pd.DataFrame(
            self.fitted_scaler_.transform(X_to_scale),
            columns=X_to_scale.columns
        )
        
        # –ö–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        # –î–ª—è transform –Ω—É–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Ç–æ—Ç –∂–µ –ø–æ–¥—Ö–æ–¥
        X_encode = pd.get_dummies(X_category, columns=['hearing(left)', 'hearing(right)', 'Urine protein', 'dental caries'])
        
        # –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ
        scaled_X = scaled_X.reset_index(drop=True)
        X_encode = X_encode.reset_index(drop=True)
        
        result = pd.concat([X_encode, scaled_X], axis=1)
        return result


def data_preprocessing(X_train, y_train, X_test, scaler=None, category_cols=None, do_category_encoding=True):
    """–ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö (—Ç–∞ –∂–µ –ª–æ–≥–∏–∫–∞, —á—Ç–æ –≤ Best —Ñ–∞–π–ª–∞—Ö)"""
    create_extra_features(X_train)
    create_extra_features(X_test)

    # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –Ω–∞ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ –∏ —á–∏—Å–ª–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
    train_to_scale = X_train.drop(category_cols, axis=1)
    train_category = X_train[category_cols]
    test_to_scale = X_test.drop(category_cols, axis=1)
    test_category = X_test[category_cols]

    if scaler is None:
        scaler = RobustScaler()

    # –°—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö (—Ç–æ–ª—å–∫–æ —á–∏—Å–ª–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏)
    scaled_train = pd.DataFrame(scaler.fit_transform(train_to_scale), columns=train_to_scale.columns)
    scaled_test = pd.DataFrame(scaler.transform(test_to_scale), columns=test_to_scale.columns)

    # –ö–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    train_encode, test_encode = category_encoding(train_category, test_category)

    # –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –∏ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–æ–≤–∞–Ω–Ω—ã—Ö —á–∏—Å–ª–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    train_encode = train_encode.reset_index(drop=True)
    test_encode = test_encode.reset_index(drop=True)
    scaled_train = scaled_train.reset_index(drop=True)
    scaled_test = scaled_test.reset_index(drop=True)

    train_df = pd.concat([train_encode, scaled_train], axis=1)
    test_df = pd.concat([test_encode, scaled_test], axis=1)

    return train_df, test_df


# ============================================================================
# –õ–£–ß–®–ò–ï –ü–ê–†–ê–ú–ï–¢–†–´ –ò–ó BEST –§–ê–ô–õ–û–í
# ============================================================================

# –ò–∑ Best_LGB.py
LGB_PARAMS = {
    'learning_rate': 0.01184431975182039,
    'num_leaves': 245,
    'max_depth': 10,
    'min_child_samples': 32,
    'subsample': 0.6624074561769746,
    'colsample_bytree': 0.662397808134481,
    'reg_alpha': 2.5502648504032812e-08,
    'reg_lambda': 0.011567327199145964,
    'n_estimators': 2083,
    'objective': 'binary',
    'metric': 'auc',
    'n_jobs': -1,
    'verbosity': -1
}

# –ò–∑ Best_XG_boost.py (–∞–∫—Ç—É–∞–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã)
XGB_PARAMS = {
    'learning_rate': 0.017940848436017145,
    'max_depth': 11,
    'min_child_weight': 60,
    'subsample': 0.9542993050541952,
    'colsample_bytree': 0.21497203607822757,
    'colsample_bylevel': 0.8724464985284567,
    'reg_alpha': 0.002852523609332756,
    'reg_lambda': 0.1462651585929734,
    'tree_method': 'hist',
    'eval_metric': 'auc',
    'n_estimators': 3000,
    'n_jobs': -1,
    'verbosity': 0,
    'random_state': 42
}

# –ò–∑ Best_CAT.py
CAT_PARAMS = {
    'learning_rate': 0.04056956101904861,
    'depth': 7,
    'l2_leaf_reg': 7.459199917293563,
    'border_count': 230,
    'bagging_temperature': 0.44856780106647864,
    'random_strength': 2.333989054467297,
    'subsample': 0.7742421038427931,
    'colsample_bylevel': 0.7364181387936571,
    'iterations': 2390,
    'min_data_in_leaf': 11,
    'loss_function': 'Logloss',
    'eval_metric': 'AUC',
    'random_state': 42,
    'thread_count': -1,
    'verbose': False,
    'allow_writing_files': False
}


# ============================================================================
# –°–¢–ï–ö–ò–ù–ì –° –ú–ï–¢–ê-–ú–û–î–ï–õ–¨–Æ
# ============================================================================

def create_meta_features(X_train, y_train, models_config, cv=5):
    """
    –°–æ–∑–¥–∞–µ—Ç –º–µ—Ç–∞-–ø—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è —Å—Ç–µ–∫–∏–Ω–≥–∞ (out-of-fold predictions)
    
    """
    
    skf = StratifiedKFold(n_splits=cv, shuffle=True, random_state=42)
    n_samples = len(X_train)
    n_models = len(models_config)
    meta_features = np.zeros((n_samples, n_models))
    

    print(f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ñ–æ–ª–¥–æ–≤ (K): {cv}")
    print(f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –º–æ–¥–µ–ª–µ–π: {n_models}")
    print(f"\n–ü—Ä–æ—Ü–µ—Å—Å:")
    print(f"  –î–ª—è –∫–∞–∂–¥–æ–π –º–æ–¥–µ–ª–∏:")
    print(f"    - –ü—Ä–æ—Ö–æ–¥–∏–º –ø–æ {cv} —Ñ–æ–ª–¥–∞–º")
    print(f"    - –ù–∞ –∫–∞–∂–¥–æ–º —Ñ–æ–ª–¥–µ: –æ–±—É—á–∞–µ–º –Ω–∞ train ‚Üí –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –Ω–∞ val")
    print(f"    - –ü–æ–ª—É—á–∞–µ–º out-of-fold –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –¥–ª—è –≤—Å–µ—Ö –æ–±—Ä–∞–∑—Ü–æ–≤")
    
    # ========================================================================
    # –î–ª—è –∫–∞–∂–¥–æ–π –º–æ–¥–µ–ª–∏ i –ø–æ–ª—É—á–∞–µ–º out-of-fold –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
    # ========================================================================
    for model_idx, model_config in enumerate(models_config):
        model_name = model_config['name']
        model_class = model_config['class']
        model_params = model_config['params']
        
        print(f"\n{'‚îÄ'*70}")
        print(f"–ú–æ–¥–µ–ª—å {model_idx+1}/{n_models}: {model_name.upper()}")
        print(f"{'‚îÄ'*70}")
        
        # –í–µ–∫—Ç–æ—Ä –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π –º–æ–¥–µ–ª–∏ i –¥–ª—è –≤—Å–µ—Ö –æ–±—Ä–∞–∑—Ü–æ–≤
        model_predictions = np.zeros(n_samples)
        
        # ====================================================================
        # K-Fold —Å—Ö–µ–º–∞: –ø–æ–ª—É—á–∞–µ–º –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ñ–æ–ª–¥–∞
        # ====================================================================
        for fold, (train_idx, val_idx) in enumerate(skf.split(X_train, y_train), 1):
            X_tr, X_val = X_train.iloc[train_idx], X_train.iloc[val_idx]
            y_tr, y_val = y_train.iloc[train_idx], y_train.iloc[val_idx]
            
            # –®–ê–ì 1: –û–±—É—á–∞–µ–º –º–æ–¥–µ–ª—å i –Ω–∞ train —á–∞—Å—Ç–∏ —Ñ–æ–ª–¥–∞ k
            model = model_class(**model_params)
            
            # –°–ø–µ—Ü–∏–∞–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–ª—è CatBoost
            if isinstance(model, CatBoostClassifier):
                model.fit(X_tr, y_tr, verbose=False)
            else:
                model.fit(X_tr, y_tr)
            
            # –®–ê–ì 2: –ü–æ–ª—É—á–∞–µ–º –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –Ω–∞ validation —á–∞—Å—Ç–∏ —Ñ–æ–ª–¥–∞ k
            if hasattr(model, 'predict_proba'):
                pred = model.predict_proba(X_val)[:, 1]  # –í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ–≥–æ –∫–ª–∞—Å—Å–∞
            else:
                pred = model.predict(X_val)
            
            # –®–ê–ì 3: –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –≤ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–µ –ø–æ–∑–∏—Ü–∏–∏
            # model_predictions[val_idx] —Å–æ–¥–µ—Ä–∂–∏—Ç –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –¥–ª—è –æ–±—Ä–∞–∑—Ü–æ–≤ –∏–∑ validation —Ñ–æ–ª–¥–∞
            model_predictions[val_idx] = pred
            
            # –í—ã—á–∏—Å–ª—è–µ–º AUC –Ω–∞ —ç—Ç–æ–º —Ñ–æ–ª–¥–µ –¥–ª—è –∫–æ–Ω—Ç—Ä–æ–ª—è –∫–∞—á–µ—Å—Ç–≤–∞
            fold_auc = roc_auc_score(y_val, pred)
            print(f"  Fold {fold}/{cv}: Train={len(train_idx)}, Val={len(val_idx)}, AUC={fold_auc:.6f}")
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –º–æ–¥–µ–ª–∏ i –≤ —Å—Ç–æ–ª–±–µ—Ü i –º–∞—Ç—Ä–∏—Ü—ã –º–µ—Ç–∞-–ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        meta_features[:, model_idx] = model_predictions
        
        # –ò—Ç–æ–≥–æ–≤–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –º–æ–¥–µ–ª–∏ i
        overall_auc = roc_auc_score(y_train, model_predictions)
        print(f"\n  ‚úì {model_name.upper()} –∑–∞–≤–µ—Ä—à–µ–Ω–∞")
        print(f"    OOF AUC (out-of-fold): {overall_auc:.6f}")
        print(f"    –î–∏–∞–ø–∞–∑–æ–Ω –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π: [{model_predictions.min():.4f}, {model_predictions.max():.4f}]")
        print(f"    –°—Ä–µ–¥–Ω–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ: {model_predictions.mean():.4f}")
    
    
    print(f"–§–æ—Ä–º–∞ –º–∞—Ç—Ä–∏—Ü—ã –º–µ—Ç–∞-–ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {meta_features.shape}")
    print(f"  - –°—Ç—Ä–æ–∫–∏: {meta_features.shape[0]} –æ–±—Ä–∞–∑—Ü–æ–≤")
    print(f"  - –°—Ç–æ–ª–±—Ü—ã: {meta_features.shape[1]} –º–æ–¥–µ–ª–µ–π")
    print(f"  - meta_features[i, j] = –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ j –¥–ª—è –æ–±—Ä–∞–∑—Ü–∞ i")
    
    # –í—ã—á–∏—Å–ª—è–µ–º –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ–º –º–µ—Ç—Ä–∏–∫–∏ –¥–ª—è –∫–∞–∂–¥–æ–π –º–æ–¥–µ–ª–∏
    model_metrics = {}
    for model_idx, model_config in enumerate(models_config):
        model_name = model_config['name']
        model_predictions = meta_features[:, model_idx]
        model_auc = roc_auc_score(y_train, model_predictions)
        model_metrics[model_name] = model_auc
    
    return meta_features, model_metrics


def train_stacking_ensemble(X_train, y_train, X_test, models_config, meta_model=None, cv=5):
    """
    –û–±—É—á–∞–µ—Ç —Å—Ç–µ–∫–∏–Ω–≥ –∞–Ω—Å–∞–º–±–ª—å
    
    Args:
        X_train: –æ–±—É—á–∞—é—â–∏–µ –¥–∞–Ω–Ω—ã–µ (—É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ)
        y_train: —Ü–µ–ª–µ–≤–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è
        X_test: —Ç–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ (—É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ)
        models_config: —Å–ø–∏—Å–æ–∫ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–π –±–∞–∑–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π
        meta_model: –º–µ—Ç–∞-–º–æ–¥–µ–ª—å (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é LogisticRegression)
        cv: –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ñ–æ–ª–¥–æ–≤ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –º–µ—Ç–∞-–ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    
    Returns:
        ensemble_pred: –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –Ω–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        fitted_base_models: —Å–ø–∏—Å–æ–∫ –æ–±—É—á–µ–Ω–Ω—ã—Ö –±–∞–∑–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π
        fitted_meta_model: –æ–±—É—á–µ–Ω–Ω–∞—è –º–µ—Ç–∞-–º–æ–¥–µ–ª—å
        meta_features_train: –º–µ—Ç–∞-–ø—Ä–∏–∑–Ω–∞–∫–∏ –Ω–∞ train –¥–∞–Ω–Ω—ã—Ö
        base_models_metrics: —Å–ª–æ–≤–∞—Ä—å —Å –º–µ—Ç—Ä–∏–∫–∞–º–∏ –±–∞–∑–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π {'lgb': auc, 'xgb': auc, ...}
        meta_model_auc: AUC –º–µ—Ç–∞-–º–æ–¥–µ–ª–∏ –Ω–∞ train –¥–∞–Ω–Ω—ã—Ö
    """
    
    if meta_model is None:
        meta_model = LogisticRegression(random_state=42, max_iter=1000)
    
    # 1. –°–æ–∑–¥–∞–µ–º –º–µ—Ç–∞-–ø—Ä–∏–∑–Ω–∞–∫–∏ –Ω–∞ train –¥–∞–Ω–Ω—ã—Ö (out-of-fold)
    meta_features_train, base_models_metrics = create_meta_features(X_train, y_train, models_config, cv=cv)
    
    # 2. –û–±—É—á–∞–µ–º –º–µ—Ç–∞-–º–æ–¥–µ–ª—å –Ω–∞ –º–µ—Ç–∞-–ø—Ä–∏–∑–Ω–∞–∫–∞—Ö
    
    meta_model.fit(meta_features_train, y_train)
    meta_model_auc = roc_auc_score(y_train, meta_model.predict_proba(meta_features_train)[:, 1])
    
    print(f"\n‚úì –ú–µ—Ç–∞-–º–æ–¥–µ–ª—å –æ–±—É—á–µ–Ω–∞")
    print(f"‚úì AUC –º–µ—Ç–∞-–º–æ–¥–µ–ª–∏ –Ω–∞ train (–º–µ—Ç–∞-–ø—Ä–∏–∑–Ω–∞–∫–∞—Ö): {meta_model_auc:.6f}")
    print(f"\n–ú–µ—Ç–∞-–º–æ–¥–µ–ª—å –Ω–∞—É—á–∏–ª–∞—Å—å –∫–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞—Ç—å –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –±–∞–∑–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π!")
    
    # 3. –û–±—É—á–∞–µ–º –±–∞–∑–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ –Ω–∞ –≤—Å–µ—Ö train –¥–∞–Ω–Ω—ã—Ö
    print(f"{'='*70}")
    fitted_base_models = []
    
    for model_config in models_config:
        model_name = model_config['name']
        model_class = model_config['class']
        model_params = model_config['params']
        
        print(f"–û–±—É—á–µ–Ω–∏–µ {model_name.upper()}...")
        model = model_class(**model_params)
        
        if isinstance(model, CatBoostClassifier):
            model.fit(X_train, y_train, verbose=False)
        else:
            model.fit(X_train, y_train)
        
        fitted_base_models.append(model)
        print(f"  ‚úì {model_name.upper()} –≥–æ—Ç–æ–≤")
    
    # 4. –°–æ–∑–¥–∞–µ–º –º–µ—Ç–∞-–ø—Ä–∏–∑–Ω–∞–∫–∏ –Ω–∞ test –¥–∞–Ω–Ω—ã—Ö
    
    meta_features_test = np.zeros((len(X_test), len(fitted_base_models)))
    
    for model_idx, model in enumerate(fitted_base_models):
        model_name = models_config[model_idx]['name']
        if hasattr(model, 'predict_proba'):
            pred = model.predict_proba(X_test)[:, 1]
        else:
            pred = model.predict(X_test)
        meta_features_test[:, model_idx] = pred
        print(f"  ‚úì {model_name.upper()}: –¥–∏–∞–ø–∞–∑–æ–Ω [{pred.min():.4f}, {pred.max():.4f}]")
    
    # 5. –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –º–µ—Ç–∞-–º–æ–¥–µ–ª–∏
    
    ensemble_pred = meta_model.predict_proba(meta_features_test)[:, 1]
    print(f"‚úì –§–∏–Ω–∞–ª—å–Ω—ã–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –≥–æ—Ç–æ–≤—ã")
    print(f"  –î–∏–∞–ø–∞–∑–æ–Ω: [{ensemble_pred.min():.4f}, {ensemble_pred.max():.4f}]")
    print(f"  –°—Ä–µ–¥–Ω–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ: {ensemble_pred.mean():.4f}")
    
    return ensemble_pred, fitted_base_models, meta_model, meta_features_train, base_models_metrics, meta_model_auc

def main():
    
    # ========================================================================
    # –®–ê–ì 1: DATA COLLECTION & SPLIT
    # ========================================================================
    
    X_train, y_train, X_test, test_ids = get_input()
    
    print(f"‚úì –†–∞–∑–º–µ—Ä –æ–±—É—á–∞—é—â–µ–π –≤—ã–±–æ—Ä–∫–∏: {X_train.shape}")
    print(f"‚úì –†–∞–∑–º–µ—Ä —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–∏: {X_test.shape}")
    print(f"‚úì –†–∞–∑–º–µ—Ä —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π: {y_train.shape}")
    print(f"‚úì –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤: {np.bincount(y_train)}")
    
    # ========================================================================
    # –®–ê–ì 2: DATA PREPROCESSING
    # ========================================================================
    
    category_cols = ['hearing(left)', 'hearing(right)', 'Urine protein', 'dental caries']
    
    X_train_processed, X_test_processed = data_preprocessing(
        X_train, y_train, X_test,
        scaler=None,  # –ò—Å–ø–æ–ª—å–∑—É–µ–º RobustScaler –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
        category_cols=category_cols,
        do_category_encoding=True
    )
    
    print(f"‚úì –ü–æ—Å–ª–µ Feature Engineering: {X_train_processed.shape[1]} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")
    print(f"‚úì –§–æ—Ä–º–∞ train: {X_train_processed.shape}")
    print(f"‚úì –§–æ—Ä–º–∞ test: {X_test_processed.shape}")
    
    # ========================================================================
    # –®–ê–ì 3: –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø –ë–ê–ó–û–í–´–• –ú–û–î–ï–õ–ï–ô
    # ========================================================================
   
    
    models_config = [
        {
            'name': 'lgb',
            'class': lgb.LGBMClassifier,
            'params': LGB_PARAMS
        },
        {
            'name': 'xgb',
            'class': xgb.XGBClassifier,
            'params': XGB_PARAMS
        },
        {
            'name': 'cat',
            'class': CatBoostClassifier,
            'params': CAT_PARAMS
        }
    ]
    
    print(f"‚úì –ù–∞—Å—Ç—Ä–æ–µ–Ω–æ –º–æ–¥–µ–ª–µ–π: {len(models_config)}")
    for model_config in models_config:
        print(f"  - {model_config['name'].upper()}")
    
    # ========================================================================
    # –®–ê–ì 4: –û–ë–£–ß–ï–ù–ò–ï –°–¢–ï–ö–ò–ù–ì –ê–ù–°–ê–ú–ë–õ–Ø
    # ========================================================================
    ensemble_pred, fitted_base_models, meta_model, meta_features_train, base_models_metrics, meta_model_auc = train_stacking_ensemble(
        X_train_processed, y_train, X_test_processed,
        models_config,
        meta_model=LogisticRegression(random_state=42, max_iter=1000),
        cv=5
    )
    
    # ========================================================================
    # –®–ê–ì 5: –°–û–•–†–ê–ù–ï–ù–ò–ï SUBMISSION
    # ========================================================================
    
    
    submission = pd.DataFrame({
        'id': test_ids,
        'smoking': ensemble_pred
    })
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç–∏
    assert len(submission) == len(test_ids), "–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–æ–∫ –Ω–µ —Å–æ–≤–ø–∞–¥–∞–µ—Ç!"
    assert submission['smoking'].min() >= 0, "–ï—Å—Ç—å –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏!"
    assert submission['smoking'].max() <= 1, "–ï—Å—Ç—å –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –±–æ–ª—å—à–µ 1!"
    assert submission['smoking'].isnull().sum() == 0, "–ï—Å—Ç—å –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è!"
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ submission —Ñ–∞–π–ª–∞
    submission_filename = 'submission_stacking.csv'
    submission.to_csv(submission_filename, index=False)
    
    print(f"‚úì Submission —Ñ–∞–π–ª —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {submission_filename}")
    print(f"‚úì –†–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞: {submission.shape[0]} —Å—Ç—Ä–æ–∫")
    
    # ========================================================================
    # –ò–¢–û–ì–û–í–ê–Ø –ò–ù–§–û–†–ú–ê–¶–ò–Ø –ò –ú–ï–¢–†–ò–ö–ò
    # ========================================================================
    
    print("\nüìä –ú–ï–¢–†–ò–ö–ò –ë–ê–ó–û–í–´–• –ú–û–î–ï–õ–ï–ô (OOF - out-of-fold):")
    print("-" * 70)
    for model_name, auc in sorted(base_models_metrics.items(), key=lambda x: x[1], reverse=True):
        print(f"  {model_name.upper():8s}: {auc:.6f}")
    
    print(f"\nüìä –ú–ï–¢–†–ò–ö–ê –ú–ï–¢–ê-–ú–û–î–ï–õ–ò ({type(meta_model).__name__}):")
    print("-" * 70)
    print(f"  –ú–µ—Ç–∞-–º–æ–¥–µ–ª—å: {meta_model_auc:.6f}")
    
    # –í—ã—á–∏—Å–ª—è–µ–º —É–ª—É—á—à–µ–Ω–∏–µ
    best_base_auc = max(base_models_metrics.values())
    improvement = meta_model_auc - best_base_auc
    improvement_pct = (improvement / best_base_auc) * 100
    
    print(f"\nüìà –°–†–ê–í–ù–ï–ù–ò–ï:")
    print("-" * 70)
    print(f"  –õ—É—á—à–∞—è –±–∞–∑–æ–≤–∞—è –º–æ–¥–µ–ª—å: {max(base_models_metrics.items(), key=lambda x: x[1])[0].upper()}")
    print(f"  AUC –ª—É—á—à–µ–π –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª–∏: {best_base_auc:.6f}")
    print(f"  AUC –º–µ—Ç–∞-–º–æ–¥–µ–ª–∏:         {meta_model_auc:.6f}")
    print(f"  –£–ª—É—á—à–µ–Ω–∏–µ:               {improvement:+.6f} ({improvement_pct:+.3f}%)")
    
    if improvement > 0:
        print(f"  ‚úì –ú–µ—Ç–∞-–º–æ–¥–µ–ª—å —É–ª—É—á—à–∏–ª–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç!")
    else:
        print(f"  ‚ö† –ú–µ—Ç–∞-–º–æ–¥–µ–ª—å –Ω–µ —É–ª—É—á—à–∏–ª–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç (–≤–æ–∑–º–æ–∂–Ω–æ, –Ω—É–∂–Ω–∞ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞)")
    
    
    print(f"–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–æ –±–∞–∑–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π: {len(fitted_base_models)}")
    print(f"–ú–µ—Ç–∞-–º–æ–¥–µ–ª—å: {type(meta_model).__name__}")
    print(f"Submission —Ñ–∞–π–ª: {submission_filename}")
    print(f"–î–∏–∞–ø–∞–∑–æ–Ω –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π: [{ensemble_pred.min():.4f}, {ensemble_pred.max():.4f}]")
    print(f"–°—Ä–µ–¥–Ω–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ: {ensemble_pred.mean():.4f}")
    print("\n" + "="*70)
    print("–ò–¢–û–ì–û–í–ê–Ø –ú–ï–¢–†–ò–ö–ê –°–¢–ï–ö–ò–ù–ì–ê:")
    print(f"  ROC-AUC: {meta_model_auc:.6f}")
    print("="*70)


if __name__ == '__main__':
    main()

